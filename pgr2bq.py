import argparse
import csv
import json
import re

from fastavro import parse_schema
from fastavro import writer as avro_writer
from geomet import wkb, wkt


def convert_wkb_to_wkt(strip_srid=True):
    def func(wkb_hex_value):
        geom = wkb.loads(bytes.fromhex(wkb_hex_value))
        if strip_srid:
            geom.pop("meta")
            geom.pop("crs")
        return wkt.dumps(geom)
    return func


class Schema:
    def __init__(self, type_converter, avro_data_type, null_value="null"):
        self._type_converter = type_converter
        self._avro_data_type = avro_data_type
        self._null_value = null_value

    def __call__(self, value):
        if value == self._null_value:
            return None
        return self._type_converter(value)

    def __str__(self):
        return self._avro_data_type

    @property
    def avro_type(self):
        return ["null", self._avro_data_type]



TABLE_SCHEMA = {
    "id": Schema(int, "int"),
    "osm_id": Schema(int, "long"),
    "osm_name": Schema(str, "string"),
    "osm_meta": Schema(str, "string"),
    "osm_source_id": Schema(int, "long"),
    "osm_target_id": Schema(int, "long"),
    "clazz": Schema(int, "int"),
    "flags": Schema(int, "int"),
    "source": Schema(int, "int"),
    "target": Schema(int, "int"),
    "km": Schema(float, "double"),
    "kmh": Schema(int, "int"),
    "cost": Schema(float, "double"),
    "reverse_cost": Schema(float, "double"),
    "x1": Schema(float, "double"),
    "y1": Schema(float, "double"),
    "x2": Schema(float, "double"),
    "y2": Schema(float, "double"),
    "geom_way": Schema(convert_wkb_to_wkt(strip_srid=True), "string")
}


def main(input_file, output_file, output_format):
    # Find and convert values after INSERT INTO ... VALUES to CSV
    records = []
    csv.register_dialect("sql", skipinitialspace=True, quotechar="'")
    with open(input_file, "r") as in_fp:
        for line in in_fp:
            insert_line = re.search(r"^\((.*?)\),?$", line)
            if insert_line:
                comma_seperated_line = insert_line.group(1)
                csv_data = csv.DictReader([comma_seperated_line], fieldnames=TABLE_SCHEMA.keys(), dialect="sql")
                data = next(csv_data)
                data = {k: TABLE_SCHEMA[k](v) for k, v in data.items()}
                records.append(data)
    with open(output_file, "wb") as out_fp:
        if output_format == "csv":
            csv_writer = csv.DictWriter(out_fp, fieldnames=TABLE_SCHEMA.keys())
            csv_writer.writeheader()
            csv_writer.writerows(records)
        if output_format == "json":
            for record in records:
                out_fp.write(json.dumps(record) + "\n")
        if output_format == "avro":
            fields = [{"name": k, "type": v.avro_type} for k, v in TABLE_SCHEMA.items()]
            schema = {
                'doc': 'PgRouting compatible way data for OSM.',
                'name': 'PgRouting Data',
                'namespace': 'osm2po',
                'type': 'record',
                'fields': fields,
            }
            parsed_schema = parse_schema(schema)
            avro_writer(out_fp, parsed_schema, records)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="sample.sql", help="Path to PgRouting SQL file generated by osm2po")
    parser.add_argument("--output", default=None, help="Output file")
    parser.add_argument("--format", default="csv", help="Output format: JSON (default), CSV or Avro")
    args = parser.parse_args()

    input_file = args.input
    output_file = args.output
    output_format = args.format
    if not args.output:
        filename, _, ext = input_file.rpartition(".")
        output_file = filename + "." + output_format.lower()
    main(input_file, output_file, output_format)
